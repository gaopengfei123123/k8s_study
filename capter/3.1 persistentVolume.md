# PersistentVolume  持久化存储卷

虽然有很多迷惑, 但是先实操一把, 随后对其中的关键点进行解析

文章阅读顺序需要从上往下看, 跳着看容易出现摸不到头脑的问题


## pv 申请存储空间:

```yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: host-10m-pv
spec:
  capacity:
    storage: 10Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  mountOptions:
    - hard
    - nfsvers=4.1
  nfs:
    path: /tmp
    server: 10.211.55.10
  #hostPath: 
    #path: /tmp/host-10m-pv/
```

这里需要注意, 虽然设置挂载方式时使用本地的临时文件,例如把上面`nfs`的配置改成`hostPath`这样, 但是实际情况中不能这么用, 
因为pv也和 pod 类似, 会随机分配到符合条件的 node节点中, 那么一旦服务器重启就无法判定这个临时目录具体跑到哪台机器上了.

当然世事无绝对, 专门设置一个临时文件节点, 然后让所有类似的 pv 都指定的部署到这个节点上也不是不可以




```sh
# 部署
kubectl apply -f host-10m-pv.yaml

# 查看状态
kubectl get pv

# 详情
kubectl  describe pv host-10m-pv
```


## pvc 对持久化卷进行绑定

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: host-5m-pvc
spec:
  storageClassName: host-test
  resources:
    requests:
      storage: 5Mi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce

```

```sh
# 部署
kubectl apply -f host-5m-pvc.yaml

# 查看状态
kubectl get pvc

# 详情
kubectl  describe pvc host-5m-pvc
```

> 这里需要注意,  两个的 `storageClassName` 需要保持一致, 不然绑定不上


从配置文件中也可以看出来, pv 是用来将物理存储统一的抽象化出来,对上层暴露出标准的接口, 底层的驱动是什么上层不需要关心
pvc 就是负责按上层业务方的需求在 k8s 中寻找符合条件的 pv, 其中 `StorageClass` 就和命名空间一样, 把资源的发布/寻找限定在一个范围当中



## 将 pvc 挂在到 pod 上

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: host-pvc-pod
  labels:
    name: host-pvc-pod
spec:
  volumes:
  - name: host-pvc-vol
    persistentVolumeClaim:
      claimName: host-5m-pvc

  containers:
  - name: nginx-pvc-pod
    image: nginx:alpine
    resources:
      limits:
        memory: "128Mi"
        cpu: "500m"
    ports:
      - containerPort: 80
    volumeMounts:
      - name: host-pvc-vol
        mountPath: /tmp

```


### 部署故障排查

如果这个时候部署, 可能会出错, 为什么?  先查看一下 pod 的部署状态

```sh
kubectl describe pod host-pvc-pod
Name:         host-pvc-pod
Namespace:    default
Priority:     0
Node:         k8s-node1/10.211.55.11
Start Time:   Tue, 02 Jan 2024 15:50:27 +0800
Labels:       name=host-pvc-pod
Annotations:  <none>
Status:       Pending
IP:
IPs:          <none>
Containers:
  nginx-pvc-pod:
    Container ID:
    Image:          nginx:alpine
    Image ID:
    Port:           80/TCP
    Host Port:      0/TCP
    State:          Waiting
      Reason:       ContainerCreating
    Ready:          False
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  128Mi
    Requests:
      cpu:        500m
      memory:     128Mi
    Environment:  <none>
    Mounts:
      /tmp from host-pvc-vol (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-4vrvk (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             False
  ContainersReady   False
  PodScheduled      True
Volumes:
  host-pvc-vol:
    Type:       PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
    ClaimName:  host-5m-pvc
    ReadOnly:   false
  kube-api-access-4vrvk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Guaranteed
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason       Age                 From               Message
  ----     ------       ----                ----               -------
  Normal   Scheduled    2m16s               default-scheduler  Successfully assigned default/host-pvc-pod to k8s-node1
  Warning  FailedMount  12s                 kubelet            Unable to attach or mount volumes: unmounted volumes=[host-pvc-vol], unattached volumes=[host-pvc-vol kube-api-access-4vrvk]: timed out waiting for the condition
  Warning  FailedMount  7s (x9 over 2m15s)  kubelet            MountVolume.SetUp failed for volume "host-10m-pv" : mount failed: exit status 32
Mounting command: mount
Mounting arguments: -t nfs -o hard,nfsvers=4.1 172.17.0.2:/tmp /var/lib/kubelet/pods/4a92aefd-5ea3-4ad9-bc88-1ff52301feed/volumes/kubernetes.io~nfs/host-10m-pv
Output: mount: /var/lib/kubelet/pods/4a92aefd-5ea3-4ad9-bc88-1ff52301feed/volumes/kubernetes.io~nfs/host-10m-pv: bad option; for several filesystems (e.g. nfs, cifs) you might need a /sbin/mount.<type> helper program.
```

看最后`Events` 部分反馈的信息来说, 就是咱们的 pv 始终请求不通 `nfsvers=4.1 172.17.0.2` 这个地址, 那是肯定的, 因为我就没在这个 ip 下安装nfs server, 也没在部署的节点上安装 nfs 的 client,  现在主要做的分以下几个步骤:
1. 安装nfs-server
2. 各node机器上安装 nfs-client
3. 依次删除 host-pvc-pod > host-5m-pvc > host-10m-pv
4. 重新部署 host-10m-pv > host-5m-pvc > host-pvc-pod


一步一步来:

#### 1. 安装nfs-server
在k8s 网络内, 找一台机器, 部署nfs-server

```sh
# 安装客户端
sudo apt -y install nfs-kernel-server

# 创建nfs 共享目录
mkdir -p /tmp/nfs

# 配置nfs 共享目录
sudo vi /etc/exports

# 注意, 这个 ip 地址就是nfs 本地的 ip 地址
/tmp/nfs 10.211.55.10/24(rw,sync,no_subtree_check,no_root_squash,insecure)
```

改动后让配置生效
```sh
sudo exportfs -ra
sudo exportfs -v

# 查看效果
exportfs -v

# 开机启动
sudo systemctl start nfs-server
sudo systemctl enable nfs-server
sudo systemctl status nfs-server


# 查看nfs 网络挂载情况
showmount -e 127.0.0.1

Export list for 127.0.0.1:
/tmp/nfs 10.211.55.10/24
```


#### 2. 各节点安装 nfs 客户端

各节点执行命令:
```sh
sudo apt -y install nfs-common
```

手动尝试节点是否挂载成功:
```sh
mkdir -p /tmp/test

# 手动绑定挂载目录
sudo mount -t nfs 10.211.55.10:/tmp/nfs /tmp/test

# 创建一个文件测试一下
touch /tmp/test/x.yml
```

这时候查看 nfs服务器上 `/tmp/nfs` 目录下是否存在

#### 3. 重新部署

我自己测试的时候, 发现直接执行删除命令的时候, pv会因为已经被分配出去, 挂载上了, 一致移除后重建

```sh
kubectl delete -f host-10m-pv.yaml
```

因此还是需要一层层的把应用层的配置都给删掉




# storageClass 动态创建 PV


## 安装 nfs-provis

为了能根据 pvc 自动创建并绑定 pv , 需要安装相关插件, 插件地址: https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner

主要关注 `deploy` 目录中三个文件:

- `rbac.yaml`  设置资源权限, 关注下namespace, 其他不用动
- `class.yaml` 配置自动 pv 的回收策略, 其中`provisioner`的配置需要和 `deployment.yaml` 中的保持一致, 参考注释
- `deployment.yaml` 这里需要关注 namespace, image, `nfs.server` 和 `nfs.path` 在国内 `registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2` 镜像不好拉取, 就替换成 `chronolaw/nfs-subdir-external-provisioner:v4.0.2`


```yml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-auto-pvc
spec:
  storageClassName:  nfs-client   # 这个名字就是 class.yaml 中 metadata.name
  accessModes:
    - ReadWriteMany
  resources: 
    requests:
      storage: 50Mi
```


# 参考资料
ref https://time.geekbang.org/column/article/547750

ref https://www.cnblogs.com/rexcheny/p/10925464.html
